{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivanandroy/T5-Finetuning-PyTorch/blob/main/notebook/T5_Fine_tuning_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbwl6E1E205R",
        "outputId": "7ab0c99b-8f25-4f48-a74b-ba54bf93c6bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
            "                                              0.0/977.5 kB ? eta -:--:--\n",
            "                                              10.2/977.5 kB ? eta -:--:--\n",
            "                                              10.2/977.5 kB ? eta -:--:--\n",
            "     -                                     30.7/977.5 kB 187.9 kB/s eta 0:00:06\n",
            "     --                                    61.4/977.5 kB 297.7 kB/s eta 0:00:04\n",
            "     ----------                             276.5/977.5 kB 1.2 MB/s eta 0:00:01\n",
            "     -----------------------------          757.8/977.5 kB 2.8 MB/s eta 0:00:01\n",
            "     -------------------------------------  972.8/977.5 kB 3.4 MB/s eta 0:00:01\n",
            "     -------------------------------------  972.8/977.5 kB 3.4 MB/s eta 0:00:01\n",
            "     -------------------------------------  972.8/977.5 kB 3.4 MB/s eta 0:00:01\n",
            "     -------------------------------------  972.8/977.5 kB 3.4 MB/s eta 0:00:01\n",
            "     -------------------------------------  972.8/977.5 kB 3.4 MB/s eta 0:00:01\n",
            "     -------------------------------------- 977.5/977.5 kB 1.9 MB/s eta 0:00:00\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Requirement already satisfied: transformers in c:\\users\\shann\\miniconda3\\lib\\site-packages (4.33.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from transformers) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from transformers) (2023.8.8)\n",
            "Requirement already satisfied: requests in c:\\users\\shann\\miniconda3\\lib\\site-packages (from transformers) (2.29.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\shann\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shann\\miniconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting rich[jupyter]\n",
            "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
            "                                              0.0/240.6 kB ? eta -:--:--\n",
            "     -                                        10.2/240.6 kB ? eta -:--:--\n",
            "     ----                                  30.7/240.6 kB 262.6 kB/s eta 0:00:01\n",
            "     -----------                           71.7/240.6 kB 563.7 kB/s eta 0:00:01\n",
            "     -------------------------------------- 240.6/240.6 kB 1.6 MB/s eta 0:00:00\n",
            "Collecting ipywidgets<9,>=7.5.1 (from rich[jupyter])\n",
            "  Using cached ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich[jupyter])\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "                                              0.0/87.5 kB ? eta -:--:--\n",
            "     ---------------------------------------- 87.5/87.5 kB ? eta 0:00:00\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from rich[jupyter]) (2.15.1)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (0.1.3)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (8.12.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (5.9.0)\n",
            "Collecting widgetsnbextension~=4.0.9 (from ipywidgets<9,>=7.5.1->rich[jupyter])\n",
            "  Using cached widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
            "Collecting jupyterlab-widgets~=3.0.9 (from ipywidgets<9,>=7.5.1->rich[jupyter])\n",
            "  Using cached jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich[jupyter])\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: backcall in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.0)\n",
            "Requirement already satisfied: decorator in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.18.2)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.1.6)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.0.38)\n",
            "Requirement already satisfied: stack-data in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.6.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.4.6)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.6)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.2.1)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\shann\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.2)\n",
            "Requirement already satisfied: six in c:\\users\\shann\\miniconda3\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.16.0)\n",
            "Installing collected packages: widgetsnbextension, mdurl, jupyterlab-widgets, markdown-it-py, rich, ipywidgets\n",
            "Successfully installed ipywidgets-8.1.1 jupyterlab-widgets-3.0.9 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.7.0 widgetsnbextension-4.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install rich[jupyter]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y6nEben93JAk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/Shivanandroy/T5-Finetuning-PyTorch/main/data/news_summary.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "Suxgy7wC4IqL",
        "outputId": "5725be18-918f-4fad-c6f4-587fd50899af"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23331</th>\n",
              "      <td>US flag replaced with Nazi flag at Wyoming park</td>\n",
              "      <td>An investigation is underway after someone rep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60219</th>\n",
              "      <td>IOC petrol pump in Nagpur gets electric chargi...</td>\n",
              "      <td>The Indian Oil Corporation, in collaboration w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75911</th>\n",
              "      <td>Vitamin B3 could reduce miscarriages, birth de...</td>\n",
              "      <td>In a first, a 12-year research by Australian s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57713</th>\n",
              "      <td>US airline honours veteran by putting his name...</td>\n",
              "      <td>United Airlines honoured veteran Mark Lehman, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93367</th>\n",
              "      <td>Emma Watson auditioned eight times for role in...</td>\n",
              "      <td>British actress Emma Watson auditioned for the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26177</th>\n",
              "      <td>IDBI Bank officers threaten 6-day strike from ...</td>\n",
              "      <td>A section of IDBI Bank officers has threatened...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28164</th>\n",
              "      <td>UP CM Yogi refuses to wear cap at Sant Kabir's...</td>\n",
              "      <td>Uttar Pradesh CM Yogi Adityanath on Wednesday ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94552</th>\n",
              "      <td>Researchers develop self-learning artificial n...</td>\n",
              "      <td>France-based researchers have developed a syna...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36972</th>\n",
              "      <td>Farmers protest as banker seeks sexual favour ...</td>\n",
              "      <td>Farmers in Maharashtra on Saturday staged a pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67957</th>\n",
              "      <td>Maharashtra plans to convert INS Viraat into n...</td>\n",
              "      <td>The Maharashtra government is planning to acqu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               headlines                                               text\n",
              "23331   US flag replaced with Nazi flag at Wyoming park   An investigation is underway after someone rep...\n",
              "60219  IOC petrol pump in Nagpur gets electric chargi...  The Indian Oil Corporation, in collaboration w...\n",
              "75911  Vitamin B3 could reduce miscarriages, birth de...  In a first, a 12-year research by Australian s...\n",
              "57713  US airline honours veteran by putting his name...  United Airlines honoured veteran Mark Lehman, ...\n",
              "93367  Emma Watson auditioned eight times for role in...  British actress Emma Watson auditioned for the...\n",
              "26177  IDBI Bank officers threaten 6-day strike from ...  A section of IDBI Bank officers has threatened...\n",
              "28164  UP CM Yogi refuses to wear cap at Sant Kabir's...  Uttar Pradesh CM Yogi Adityanath on Wednesday ...\n",
              "94552  Researchers develop self-learning artificial n...  France-based researchers have developed a syna...\n",
              "36972  Farmers protest as banker seeks sexual favour ...  Farmers in Maharashtra on Saturday staged a pr...\n",
              "67957  Maharashtra plans to convert INS Viraat into n...  The Maharashtra government is planning to acqu..."
            ]
          },
          "execution_count": 3,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AYfBicZQ59Jf"
      },
      "outputs": [],
      "source": [
        "df[\"text\"] = \"summarize: \"+df[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "81f4PKa1F6aM",
        "outputId": "fcf57854-d194-4670-c783-35da1574ec5c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "      <td>summarize: Saurav Kant, an alumnus of upGrad a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>summarize: Kunal Shah's credit card bill payme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
              "      <td>summarize: New Zealand defeated India by 8 wic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "      <td>summarize: With Aegon Life iTerm Insurance pla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "      <td>summarize: Speaking about the sexual harassmen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headlines                                               text\n",
              "0  upGrad learner switches to career in ML & Al w...  summarize: Saurav Kant, an alumnus of upGrad a...\n",
              "1  Delhi techie wins free food from Swiggy for on...  summarize: Kunal Shah's credit card bill payme...\n",
              "2  New Zealand end Rohit Sharma-led India's 12-ma...  summarize: New Zealand defeated India by 8 wic...\n",
              "3  Aegon life iTerm insurance plan helps customer...  summarize: With Aegon Life iTerm Insurance pla...\n",
              "4  Have known Hirani for yrs, what if MeToo claim...  summarize: Speaking about the sexual harassmen..."
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wB441x104K-o"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import os\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "\n",
        "# define a rich console logger\n",
        "console=Console(record=True)\n",
        "\n",
        "def display_df(df):\n",
        "  \"\"\"display dataframe in ASCII format\"\"\"\n",
        "\n",
        "  console=Console()\n",
        "  table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
        "\n",
        "  for i, row in enumerate(df.values.tolist()):\n",
        "    table.add_row(row[0], row[1])\n",
        "\n",
        "  console.print(table)\n",
        "\n",
        "training_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n",
        "                        Column(\"Steps\", justify=\"center\"),\n",
        "                        Column(\"Loss\", justify=\"center\"), \n",
        "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tlYaKW9h4ai_"
      },
      "outputs": [],
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8vLQPGAn4v17"
      },
      "outputs": [],
      "source": [
        "class YourDataSetClass(Dataset):\n",
        "  \"\"\"\n",
        "  Creating a custom dataset for reading the dataset and \n",
        "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.source_len = source_len\n",
        "    self.summ_len = target_len\n",
        "    self.target_text = self.data[target_text]\n",
        "    self.source_text = self.data[source_text]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target_text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    source_text = str(self.source_text[index])\n",
        "    target_text = str(self.target_text[index])\n",
        "\n",
        "    #cleaning data so as to ensure data is in string type\n",
        "    source_text = ' '.join(source_text.split())\n",
        "    target_text = ' '.join(target_text.split())\n",
        "\n",
        "    source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "    target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "\n",
        "    source_ids = source['input_ids'].squeeze()\n",
        "    source_mask = source['attention_mask'].squeeze()\n",
        "    target_ids = target['input_ids'].squeeze()\n",
        "    target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "    return {\n",
        "        'source_ids': source_ids.to(dtype=torch.long), \n",
        "        'source_mask': source_mask.to(dtype=torch.long), \n",
        "        'target_ids': target_ids.to(dtype=torch.long),\n",
        "        'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Nkj6wIMt40RK"
      },
      "outputs": [],
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to be called for training with the parameters passed from main function\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  model.train()\n",
        "  for _,data in enumerate(loader, 0):\n",
        "    y = data['target_ids'].to(device, dtype = torch.long)\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "    ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "    mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "    loss = outputs[0]\n",
        "\n",
        "    if _%10==0:\n",
        "      training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "      console.print(training_logger)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GUBykK-A43DF"
      },
      "outputs": [],
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to evaluate model for predictions\n",
        "\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(loader, 0):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask, \n",
        "              max_length=150, \n",
        "              num_beams=2,\n",
        "              repetition_penalty=2.5, \n",
        "              length_penalty=1.0, \n",
        "              early_stopping=True\n",
        "              )\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "          if _%10==0:\n",
        "              console.print(f'Completed {_}')\n",
        "\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "  return predictions, actuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V5L4wr3h4612"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Tw4RW_qO4_8T"
      },
      "outputs": [],
      "source": [
        "def T5Trainer(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
        "  \n",
        "  \"\"\"\n",
        "  T5 trainer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Set random seeds and deterministic pytorch for reproducibility\n",
        "  torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
        "  np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  # logging\n",
        "  console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "  # tokenzier for encoding the text\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        "\n",
        "  # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "  # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "  model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "  model = model.to(device)\n",
        "  \n",
        "  # logging\n",
        "  console.log(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "  # Importing the raw dataset\n",
        "  dataframe = dataframe[[source_text,target_text]]\n",
        "  display_df(dataframe.head(2))\n",
        "\n",
        "  \n",
        "  # Creation of Dataset and Dataloader\n",
        "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
        "  train_size = 0.8\n",
        "  train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
        "  val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "  train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "  console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "  console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "  console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "\n",
        "  # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "  training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "  val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "\n",
        "\n",
        "  # Defining the parameters for creation of dataloaders\n",
        "  train_params = {\n",
        "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "      'shuffle': True,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  val_params = {\n",
        "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
        "      'shuffle': False,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "  training_loader = DataLoader(training_set, **train_params)\n",
        "  val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "  # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "  optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
        "\n",
        "\n",
        "  # Training loop\n",
        "  console.log(f'[Initiating Fine Tuning]...\\n')\n",
        "\n",
        "  for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "      train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "      \n",
        "  console.log(f\"[Saving Model]...\\n\")\n",
        "  #Saving the model after training\n",
        "  path = os.path.join(output_dir, \"model_files\")\n",
        "  model.save_pretrained(path)\n",
        "  tokenizer.save_pretrained(path)\n",
        "\n",
        "\n",
        "  # evaluating test dataset\n",
        "  console.log(f\"[Initiating Validation]...\\n\")\n",
        "  for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "    final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
        "  \n",
        "  console.save_text(os.path.join(output_dir,'logs.txt'))\n",
        "  \n",
        "  console.log(f\"[Validation Completed.]\\n\")\n",
        "  console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
        "  console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
        "  console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PxCpQwD8PDIs"
      },
      "outputs": [],
      "source": [
        "model_params={\n",
        "    \"MODEL\":\"t5-base\",             # model_type: t5-base/t5-large\n",
        "    \"TRAIN_BATCH_SIZE\":8,          # training batch size\n",
        "    \"VALID_BATCH_SIZE\":8,          # validation batch size\n",
        "    \"TRAIN_EPOCHS\":3,              # number of training epochs\n",
        "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
        "    \"LEARNING_RATE\":1e-4,          # learning rate\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\":512,  # max length of source text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\":50,   # max length of target text\n",
        "    \"SEED\": 42                     # set seed for reproducibility \n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qijZoYeI55fM",
        "outputId": "69c68bb6-4fba-47e4-9e74-73f2579aa3c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[12:10:09] [Model]: Loading t5-base...        <ipython-input-11-0f05b878f99b>:14\n",
            "                                                                                \n",
            "[12:10:21] [Data]: Reading data...            <ipython-input-11-0f05b878f99b>:25\n",
            "                                                                                \n",
            "                                  Sample Data                                   \n",
            "+------------------------------------------------------------------------------+\n",
            "|             source_text              |              target_text              |\n",
            "|--------------------------------------+---------------------------------------|\n",
            "|summarize: Saurav Kant, an alumnus of |  upGrad learner switches to career in |\n",
            "|  upGrad and IIIT-B's PG Program in   |      ML & Al with 90% salary hike     |\n",
            "|   Machine learning and Artificial    |                                       |\n",
            "|   Intelligence, was a Sr Systems     |                                       |\n",
            "|  Engineer at Infosys with almost 5   |                                       |\n",
            "|years of work experience. The program |                                       |\n",
            "|   and upGrad's 360-degree career     |                                       |\n",
            "| support helped him transition to a   |                                       |\n",
            "|Data Scientist at Tech Mahindra with  |                                       |\n",
            "|  90% salary hike. upGrad's Online    |                                       |\n",
            "| Power Learning has powered 3 lakh+   |                                       |\n",
            "|              careers.                |                                       |\n",
            "| summarize: Kunal Shah's credit card  |    Delhi techie wins free food from   |\n",
            "|  bill payment platform, CRED, gave   |      Swiggy for one year on CRED      |\n",
            "|users a chance to win free food from  |                                       |\n",
            "|Swiggy for one year. Pranav Kaushik,  |                                       |\n",
            "| a Delhi techie, bagged this reward   |                                       |\n",
            "|after spending 2000 CRED coins. Users |                                       |\n",
            "| get one CRED coin per rupee of bill  |                                       |\n",
            "|  paid, which can be used to avail    |                                       |\n",
            "|   rewards from brands like Ixigo,    |                                       |\n",
            "| BookMyShow, UberEats, Cult.Fit and   |                                       |\n",
            "|                more.                 |                                       |\n",
            "+------------------------------------------------------------------------------+\n",
            "FULL Dataset: (500, 2)\n",
            "TRAIN Dataset: (400, 2)\n",
            "TEST Dataset: (100, 2)\n",
            "\n",
            "           [Initiating Fine Tuning]...        <ipython-input-11-0f05b878f99b>:74\n",
            "                                                                                \n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  40   | tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  40   | tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |   0   | tensor(2.2411, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  40   | tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |   0   | tensor(2.2411, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  10   | tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  40   | tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |   0   | tensor(2.2411, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  10   | tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  20   | tensor(1.9091, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  40   | tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |   0   | tensor(2.2411, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  10   | tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  20   | tensor(1.9091, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  30   | tensor(2.0122, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  40   | tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |   0   | tensor(2.2411, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  10   | tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  20   | tensor(1.9091, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  30   | tensor(2.0122, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  40   | tensor(1.5261, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  40   | tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |   0   | tensor(2.2411, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  10   | tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  20   | tensor(1.9091, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  30   | tensor(2.0122, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  40   | tensor(1.5261, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |   0   | tensor(1.6496, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  40   | tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |   0   | tensor(2.2411, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  10   | tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  20   | tensor(1.9091, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  30   | tensor(2.0122, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  40   | tensor(1.5261, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |   0   | tensor(1.6496, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |  10   | tensor(1.1971, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  40   | tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |   0   | tensor(2.2411, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  10   | tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  20   | tensor(1.9091, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  30   | tensor(2.0122, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  40   | tensor(1.5261, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |   0   | tensor(1.6496, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |  10   | tensor(1.1971, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |  20   | tensor(1.6908, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  40   | tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |   0   | tensor(2.2411, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  10   | tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  20   | tensor(1.9091, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  30   | tensor(2.0122, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  40   | tensor(1.5261, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |   0   | tensor(1.6496, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |  10   | tensor(1.1971, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |  20   | tensor(1.6908, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |  30   | tensor(1.4069, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "                              Training Status                               \n",
            "+--------------------------------------------------------------------------+\n",
            "|Epoch | Steps |                            Loss                           |\n",
            "|------+-------+-----------------------------------------------------------|\n",
            "|  0   |   0   | tensor(8.5338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  10   | tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  20   | tensor(3.0148, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  30   | tensor(3.2338, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  0   |  40   | tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |   0   | tensor(2.2411, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  10   | tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  20   | tensor(1.9091, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  30   | tensor(2.0122, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  1   |  40   | tensor(1.5261, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |   0   | tensor(1.6496, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |  10   | tensor(1.1971, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |  20   | tensor(1.6908, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |  30   | tensor(1.4069, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "|  2   |  40   | tensor(2.1261, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
            "+--------------------------------------------------------------------------+\n",
            "[12:12:58] [Saving Model]...                  <ipython-input-11-0f05b878f99b>:79\n",
            "                                                                                \n",
            "[12:13:02] [Initiating Validation]...         <ipython-input-11-0f05b878f99b>:87\n",
            "                                                                                \n",
            "Completed 0\n",
            "Completed 10\n",
            "[12:13:41] [Validation Completed.]            <ipython-input-11-0f05b878f99b>:95\n",
            "                                                                                \n",
            "[Model] Model saved @ outputs/model_files\n",
            "\n",
            "[Validation] Validation data saved @ outputs/predictions.csv\n",
            "\n",
            "[Logs] Logs saved @ outputs/logs.txt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "T5Trainer(dataframe=df[:500], source_text=\"text\", target_text=\"headlines\", model_params=model_params, output_dir=\"outputs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XD2qL87Wsn19"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "u7SFz-6usqym"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM+sqU2Hgca8RM/Wjv+9kvQ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "T5 Fine tuning with PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
